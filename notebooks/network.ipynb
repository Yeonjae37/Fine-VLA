{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1648b513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######USING ATTENTION STYLE:  frozen-in-time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Documents\\Fine-VLA\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0116\n"
     ]
    }
   ],
   "source": [
    "import autorootcwd\n",
    "from src.model.model import FrozenInTime, compute_similarity\n",
    "from src.data.data_loader import TextVideoDataLoader\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_loader = TextVideoDataLoader(\n",
    "    dataset_name=\"NTU\",\n",
    "    text_params={\"model\": \"distilbert-base-uncased\", \"pretrained\": True},\n",
    "    video_params={\"model\": \"SpaceTimeTransformer\", \"arch_config\": \"base_patch16_224\", \"num_frames\": 4, \"input_res\": 224, \"pretrained\": True, \"time_init\": \"zeros\", \"attention_style\": \"frozen-in-time\"},\n",
    "    data_dir=\"data/nturgbd\",\n",
    "    \n",
    "    batch_size=8,\n",
    "    )\n",
    "model = FrozenInTime(\n",
    "    video_params={\"model\": \"SpaceTimeTransformer\", \"arch_config\": \"base_patch16_224\", \"num_frames\": 4, \"pretrained\": True, \"time_init\": \"zeros\", \"attention_style\": \"frozen-in-time\"},\n",
    "    text_params={\"model\": \"distilbert-base-uncased\", \"pretrained\": True},\n",
    "    projection_dim=256,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "batch_sample = next(iter(train_loader))\n",
    "video_data = batch_sample['video'].to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "text_data = tokenizer(\n",
    "    batch_sample['text'],          # List[str]\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer.model_max_length\n",
    ").to(device)\n",
    "\n",
    "video_embeddings = model.compute_video(video_data)\n",
    "text_embeddings = model.compute_text(text_data)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "sim_matrix, _ = compute_similarity(text_embeddings, video_embeddings)\n",
    "_, similarity_loss = compute_similarity(video_embeddings, text_embeddings)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss = -similarity_loss.mean()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce36c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(5):\n",
    "    loss_epoch = 0.0\n",
    "    for batch_sample in train_loader:\n",
    "        video_embeddings = model.compute_video(batch_sample['video']).cuda()\n",
    "        text_embeddings = model.compute_text(batch_sample['text']).cuda()\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        similarity_loss = model.compute_similarity(video_embeddings, text_embeddings)\n",
    "        optimizer.zero_grad()\n",
    "        similarity_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += similarity_loss.item()\n",
    "\n",
    "    loss_epoch /= len(train_loader)\n",
    "\n",
    "    print(f'loss at epoch{epoch}:', loss_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_texts = [\n",
    "    \"drink_water\",\n",
    "    \"eat_meal\",\n",
    "    \"brush_teeth\",\n",
    "    \"brush_hair\",\n",
    "    \"drop\",\n",
    "    \"pickup\",\n",
    "    \"throw\",\n",
    "    \"sit_down\",\n",
    "    \"stand_up\",\n",
    "    \"clapping\",\n",
    "    \"reading\",\n",
    "    \"writing\",\n",
    "    \"hand_waving\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f39baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 준비\n",
    "sample_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "test_videos = []\n",
    "test_texts = []\n",
    "\n",
    "# 테스트 데이터 샘플 수집\n",
    "with torch.no_grad():\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        if i >= len(train_loader.dataset):\n",
    "            break\n",
    "        sample = train_loader.dataset[idx]\n",
    "        \n",
    "        # 비디오 임베딩 계산\n",
    "        video = sample['video'].unsqueeze(0).to(device)\n",
    "        test_videos.append(video)\n",
    "        \n",
    "    # 텍스트 임베딩 계산\n",
    "    text_inputs = []\n",
    "    for text in custom_texts:\n",
    "        text_input = train_loader.dataset.tokenize([text])\n",
    "        text_input = {k: v.to(device) for k, v in text_input.items()}\n",
    "        text_inputs.append(text_input)\n",
    "\n",
    "    # 임베딩 계산\n",
    "    video_embeddings = torch.cat([model.compute_video(video) for video in test_videos], dim=0)\n",
    "    text_embeddings = torch.stack([model.compute_text(text_input) for text_input in text_inputs], dim=0)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 임베딩 결합 및 NumPy 변환\n",
    "all_embeddings = torch.cat([video_embeddings, text_embeddings], dim=0).cpu().numpy()\n",
    "\n",
    "# 라벨 준비\n",
    "video_labels = [f\"비디오 {i}\" for i in sample_indices[:len(test_videos)]]\n",
    "text_labels = custom_texts\n",
    "all_labels = video_labels + text_labels\n",
    "\n",
    "# t-SNE 시각화\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "video_count = len(video_labels)\n",
    "plt.scatter(\n",
    "    embeddings_2d[:video_count, 0],\n",
    "    embeddings_2d[:video_count, 1],\n",
    "    c='blue',\n",
    "    marker='o',\n",
    "    label='비디오'\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    embeddings_2d[video_count:, 0],\n",
    "    embeddings_2d[video_count:, 1],\n",
    "    c='red',\n",
    "    marker='^',\n",
    "    label='텍스트'\n",
    ")\n",
    "\n",
    "for i, (x, y, label) in enumerate(zip(embeddings_2d[:, 0], embeddings_2d[:, 1], all_labels)):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        (x, y),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 5),\n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "plt.title('비디오와 텍스트 임베딩의 t-SNE 시각화')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 유사도 행렬 계산 및 시각화\n",
    "similarity_matrix = torch.mm(video_embeddings, text_embeddings.t()).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(similarity_matrix, cmap='viridis')\n",
    "plt.colorbar(label='코사인 유사도')\n",
    "plt.xticks(np.arange(len(text_labels)), text_labels, rotation=45, ha='right')\n",
    "plt.yticks(np.arange(len(video_labels)), video_labels)\n",
    "plt.title('비디오-텍스트 유사도 행렬')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fine-VLA",
   "language": "python",
   "name": "fine-vla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
