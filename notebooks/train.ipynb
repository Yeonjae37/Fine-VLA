{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08a64093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######USING ATTENTION STYLE:  frozen-in-time\n"
     ]
    }
   ],
   "source": [
    "import autorootcwd\n",
    "from src.model.model import FrozenInTime, compute_similarity\n",
    "from src.data.data_loader import TextVideoDataLoader\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_loader = TextVideoDataLoader(\n",
    "    dataset_name= \"NTU\",\n",
    "    text_params= {\"input\": \"text\",},\n",
    "    video_params= {\"input_res\": 224, \"num_frames\": 4,},\n",
    "    data_dir= \"data/nturgbd\",\n",
    "    metadata_dir= \"data/nturgbd\",\n",
    "    split= 'train',\n",
    "    tsfm_params= None,\n",
    "    tsfm_split= None,\n",
    "    cut= None,\n",
    "    subsample= 1,\n",
    "    sliding_window_stride= -1,\n",
    "    reader= 'decord',\n",
    "    batch_size=32,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "model = FrozenInTime(\n",
    "    video_params={\"model\": \"SpaceTimeTransformer\", \"num_frames\": 4, \"arch_config\":\"base_patch16_224\", \"vit_init\": \"imagenet-21k\", \"attention_style\":\"frozen-in-time\", \"pretrained\":True},\n",
    "    text_params={\"model\": \"distilbert-base-uncased\", \"pretrained\": True},\n",
    "    projection_dim=256,\n",
    ").to(device)\n",
    "\n",
    "for p in model.video_model.parameters(): p.requires_grad=False\n",
    "for p in model.text_model.parameters():  p.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4937b674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Documents\\Fine-VLA\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "batch = next(iter(data_loader))\n",
    "video_data = batch['video'].to(device)\n",
    "text_data = tokenizer(\n",
    "    batch['text'],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer.model_max_length\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2294530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 01 — loss: 0.0243\n",
      "Step 02 — loss: -0.5016\n",
      "Step 03 — loss: -0.6397\n",
      "Step 04 — loss: -0.7773\n",
      "Step 05 — loss: -0.8865\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i in range(5):\n",
    "    # 1. forward\n",
    "    video_embeddings = model.compute_video(video_data)\n",
    "    text_embeddings  = model.compute_text(text_data)\n",
    "\n",
    "    # 2. positive 유사도만 뽑아서 loss 정의\n",
    "    _, pos_sims = compute_similarity(video_embeddings, text_embeddings)\n",
    "    loss = - pos_sims.mean()\n",
    "\n",
    "    # 3. backward & step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Step {i+1:02d} — loss: {loss.item():.4f}\")\n",
    "    \n",
    "torch.save(model.state_dict(), \"finetuned_head.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "689cbaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 0/10 finished, average loss = 0.0041 ---\n",
      "Saved checkpoint to finetuned_epoch1.pkl\n",
      "Finished epoch 1\n",
      "\n",
      "--- Epoch 1/10 finished, average loss = 0.0031 ---\n",
      "Saved checkpoint to finetuned_epoch2.pkl\n",
      "Finished epoch 2\n",
      "\n",
      "--- Epoch 2/10 finished, average loss = 0.0038 ---\n",
      "Saved checkpoint to finetuned_epoch3.pkl\n",
      "Finished epoch 3\n",
      "\n",
      "--- Epoch 3/10 finished, average loss = 0.0035 ---\n",
      "Saved checkpoint to finetuned_epoch4.pkl\n",
      "Finished epoch 4\n",
      "\n",
      "--- Epoch 4/10 finished, average loss = 0.0047 ---\n",
      "Saved checkpoint to finetuned_epoch5.pkl\n",
      "Finished epoch 5\n",
      "\n",
      "--- Epoch 5/10 finished, average loss = 0.0031 ---\n",
      "Saved checkpoint to finetuned_epoch6.pkl\n",
      "Finished epoch 6\n",
      "\n",
      "--- Epoch 6/10 finished, average loss = 0.0039 ---\n",
      "Saved checkpoint to finetuned_epoch7.pkl\n",
      "Finished epoch 7\n",
      "\n",
      "--- Epoch 7/10 finished, average loss = 0.0038 ---\n",
      "Saved checkpoint to finetuned_epoch8.pkl\n",
      "Finished epoch 8\n",
      "\n",
      "--- Epoch 8/10 finished, average loss = 0.0039 ---\n",
      "Saved checkpoint to finetuned_epoch9.pkl\n",
      "Finished epoch 9\n",
      "\n",
      "--- Epoch 9/10 finished, average loss = 0.0028 ---\n",
      "Saved checkpoint to finetuned_epoch10.pkl\n",
      "Finished epoch 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "num_epochs = 10\n",
    "log_interval = 100\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    epoch_loss   = 0.0\n",
    "    num_batches  = 0\n",
    "    for batch_idx, batch in enumerate(data_loader, start=1):\n",
    "        video_data = batch[\"video\"].to(device)\n",
    "        text_data = tokenizer(\n",
    "            batch['text'],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=tokenizer.model_max_length\n",
    "        ).to(device)\n",
    "\n",
    "        video_embeddings = model.compute_video(video_data)\n",
    "        text_embeddings = model.compute_text(text_data)\n",
    "\n",
    "        _, pos_sims = compute_similarity(video_embeddings, text_embeddings)\n",
    "        loss = -pos_sims.mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        epoch_loss   += loss.item()\n",
    "        num_batches  += 1\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            avg = running_loss / log_interval\n",
    "            print(f\"[Epoch {epoch+1}/{num_epochs}  Batch {batch_idx}]  loss = {avg:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    epoch_avg = epoch_loss / num_batches\n",
    "    print(f\"--- Epoch {epoch}/{num_epochs} finished, average loss = {epoch_avg:.4f} ---\")\n",
    "\n",
    "    ckpt = {\"state_dict\": model.state_dict()}\n",
    "    ckpt_path = f\"finetuned_epoch{epoch+1}.pkl\"\n",
    "    with open(ckpt_path, \"wb\") as f:\n",
    "        pickle.dump(ckpt, f)\n",
    "\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")\n",
    "    print(f\"Finished epoch {epoch+1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "642f4a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1/10 avg loss = 3.4503 ---\n",
      "Saved checkpoint to finetuned_epoch1.pkl\n",
      "\n",
      "--- Epoch 2/10 avg loss = 3.4462 ---\n",
      "Saved checkpoint to finetuned_epoch2.pkl\n",
      "\n",
      "--- Epoch 3/10 avg loss = 3.4431 ---\n",
      "Saved checkpoint to finetuned_epoch3.pkl\n",
      "\n",
      "--- Epoch 4/10 avg loss = 3.4375 ---\n",
      "Saved checkpoint to finetuned_epoch4.pkl\n",
      "\n",
      "--- Epoch 5/10 avg loss = 3.4338 ---\n",
      "Saved checkpoint to finetuned_epoch5.pkl\n",
      "\n",
      "--- Epoch 6/10 avg loss = 3.4311 ---\n",
      "Saved checkpoint to finetuned_epoch6.pkl\n",
      "\n",
      "--- Epoch 7/10 avg loss = 3.4242 ---\n",
      "Saved checkpoint to finetuned_epoch7.pkl\n",
      "\n",
      "--- Epoch 8/10 avg loss = 3.4207 ---\n",
      "Saved checkpoint to finetuned_epoch8.pkl\n",
      "\n",
      "--- Epoch 9/10 avg loss = 3.4125 ---\n",
      "Saved checkpoint to finetuned_epoch9.pkl\n",
      "\n",
      "--- Epoch 10/10 avg loss = 3.4100 ---\n",
      "Saved checkpoint to finetuned_epoch10.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from src.model.text_augmentation import augment_text_labels, average_augmented_embeddings\n",
    "\n",
    "batch = next(iter(data_loader))\n",
    "video_data = batch['video'].to(device)\n",
    "text_data = tokenizer(\n",
    "    batch['text'],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer.model_max_length\n",
    ").to(device)\n",
    "\n",
    "# 1) backbone freeze\n",
    "for p in model.video_model.parameters(): p.requires_grad = False\n",
    "for p in model.text_model.parameters():  p.requires_grad = False\n",
    "\n",
    "# 2) 사전 계산: 모든 라벨에 대해 증강 텍스트 → 임베딩 → 평균\n",
    "# dataset.metadata['raw_captions'] 에 모든 캡션(라벨) 저장돼 있다고 가정\n",
    "unique_labels = sorted(set(data_loader.dataset.metadata['caption']))\n",
    "aug_data       = augment_text_labels(unique_labels)\n",
    "augmented_texts, label_groups = aug_data['augmented_texts'], aug_data['label_groups']\n",
    "\n",
    "all_aug_embeds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 한번에 너무 많이 넣으면 OOM 날 수 있으니 쪼개서\n",
    "    chunk = 32\n",
    "    for i in range(0, len(augmented_texts), chunk):\n",
    "        batch_txts = augmented_texts[i:i+chunk]\n",
    "        toks = tokenizer(batch_txts, return_tensors=\"pt\",\n",
    "                         padding=True, truncation=True,\n",
    "                         max_length=tokenizer.model_max_length).to(device)\n",
    "        emb = model.compute_text(toks)           # [chunk, dim]\n",
    "        all_aug_embeds.append(emb.cpu())\n",
    "all_aug_embeds = torch.cat(all_aug_embeds, dim=0).to(device)  # [num_templates, dim]\n",
    "\n",
    "averaged_embeds = average_augmented_embeddings(all_aug_embeds, label_groups)\n",
    "# averaged_embeds: [num_labels, dim]\n",
    "\n",
    "# 라벨→인덱스 매핑\n",
    "label_to_idx = {lbl:i for i,lbl in enumerate(unique_labels)}\n",
    "\n",
    "# 3) optimizer: head만\n",
    "optimizer = Adam(\n",
    "    list(model.vid_proj.parameters()) +\n",
    "    list(model.txt_proj.parameters()),\n",
    "    lr=1e-5\n",
    ")\n",
    "\n",
    "# 4) fine-tuning loop\n",
    "num_epochs   = 10\n",
    "log_interval = 100\n",
    "temperature  = 0.07\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    epoch_loss   = 0.0\n",
    "    n_batches    = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader, start=1):\n",
    "        video_data = batch[\"video\"].to(device)\n",
    "        # 원래 batch['text'] 가 raw caption 문자열\n",
    "        batch_labels = batch['text']\n",
    "        idxs = [label_to_idx[lbl] for lbl in batch_labels]\n",
    "        t_emb = averaged_embeds[idxs]           # [B, dim]\n",
    "\n",
    "        v_emb = model.compute_video(video_data) # [B, dim]\n",
    "\n",
    "        # contrastive InfoNCE\n",
    "        logits, _ = compute_similarity(v_emb, t_emb)\n",
    "        logits = logits / temperature\n",
    "        labels = torch.arange(len(v_emb), device=device)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        epoch_loss   += loss.item()\n",
    "        n_batches    += 1\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"[Epoch {epoch}/{num_epochs}  Batch {batch_idx}]\"\n",
    "                  f\"  batch-loss = {running_loss/log_interval:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / n_batches\n",
    "    print(f\"--- Epoch {epoch}/{num_epochs} avg loss = {avg_epoch_loss:.4f} ---\")\n",
    "\n",
    "    # 5) 체크포인트 저장\n",
    "    ckpt = {\"state_dict\": model.state_dict()}\n",
    "    with open(f\"finetuned_epoch{epoch}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ckpt, f)\n",
    "    print(f\"Saved checkpoint to finetuned_epoch{epoch}.pkl\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fine-VLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
