{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd20619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# Block 1: 라이브러리 & 설정\n",
    "# ======================\n",
    "import autorootcwd\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, CLIPTokenizer\n",
    "from scripts.parse_config import ConfigParser\n",
    "import torch.serialization\n",
    "import src.model.model as module_arch\n",
    "import src.data.data_loader as module_data\n",
    "import src.model.metric as module_metric\n",
    "from src.utils.util import state_dict_data_parallel_fix\n",
    "from src.model.model import compute_similarity\n",
    "from src.model.text_augmentation import augment_text_labels, average_augmented_embeddings\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "884ca074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Block 1: 공통 설정 =====\n",
    "import torch, torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from src.data.data_loader import TextVideoDataLoader\n",
    "from src.model.model import FrozenInTime, compute_similarity\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DL_KW = dict(\n",
    "    dataset_name=\"NTU\",\n",
    "    text_params={\"input\": \"text\"},\n",
    "    video_params={\"extraction_fps\": 25, \"extraction_res\": 320, \"input_res\": 224, \"num_frames\": 4, \"stride\": 1},\n",
    "    data_dir=\"data/nturgbd\",\n",
    "    metadata_dir=\"data/nturgbd\",\n",
    "    split=\"test\",\n",
    "    tsfm_params={\"input_res\": 224, \"center_crop\": 224},\n",
    "    tsfm_split=\"test\",\n",
    "    subsample=1,\n",
    "    sliding_window_stride=-1,\n",
    "    reader=\"decord\",\n",
    "    batch_size=8,\n",
    "    num_workers=2,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "TEXT_MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "FIT_ARGS = dict(\n",
    "    video_params=dict(model=\"SpaceTimeTransformer\",\n",
    "                      arch_config=\"base_patch16_224\",\n",
    "                      num_frames=4,\n",
    "                      vit_init=\"imagenet-21k\",\n",
    "                      attention_style=\"frozen-in-time\",\n",
    "                      pretrained=True),\n",
    "    text_params=dict(model=TEXT_MODEL_NAME, pretrained=True),\n",
    "    projection_dim=256,\n",
    "    projection='minimal',\n",
    ")\n",
    "\n",
    "VJEPA_ARGS = dict(\n",
    "    video_params=dict(model=\"VJEPA2\", num_frames=32, pretrained=True),  # hf_repo나 로컬 가중치 쓸거면 여기에 키 추가\n",
    "    text_params=dict(model=TEXT_MODEL_NAME, pretrained=True),\n",
    "    projection_dim=256,\n",
    "    projection='',  # ★ 프로젝션 끔: 원 임베딩 쓰기\n",
    ")\n",
    "\n",
    "ALPHA = 0.7  # S_fused = α*S_clip + (1-α)*S_motion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1212d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 600)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Block 2: 데이터/토크나이저 =====\n",
    "data_loader = TextVideoDataLoader(**DL_KW)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "\n",
    "# 텍스트 캡션 전체 수집 (라벨/메타도 함께)\n",
    "all_texts = []\n",
    "all_video_paths = []\n",
    "for batch in data_loader:\n",
    "    all_texts += batch['text']\n",
    "    all_video_paths += batch['meta']['paths'] if isinstance(batch['meta']['paths'], list) else [batch['meta']['paths']]\n",
    "len(all_texts), len(all_video_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4378a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######USING ATTENTION STYLE:  frozen-in-time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model = FrozenInTime(**FIT_ARGS).to(device).eval()\n",
    "vjepa_model = FrozenInTime(**VJEPA_ARGS).to(device).eval()\n",
    "\n",
    "ckpt_path = \"src/exps/pretrained/cc-webvid2m-4f_stformer_b_16_224.pth.tar\"\n",
    "import torch\n",
    "from src.utils.util import state_dict_data_parallel_fix\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location='cuda', weights_only=False)\n",
    "\n",
    "sd = state_dict_data_parallel_fix(ckpt['state_dict'], fit_model.state_dict())\n",
    "fit_model.load_state_dict(sd, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dd8da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Block 4: 임베딩 추출 (FiT: text & video, V‑JEPA2: video만) =====\n",
    "import torch.nn.functional as F\n",
    "\n",
    "txt_emb_list, vid_fit_list, vid_jepa_list, labels = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        video = batch['video'].to(device)     # (B, T, C, H, W)\n",
    "        texts = batch['text']\n",
    "        labels += batch['meta']['raw_captions'] if isinstance(batch['meta']['raw_captions'], list) else [batch['meta']['raw_captions']]\n",
    "\n",
    "        toks = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # --- FiT는 4프레임 체크포인트 가정 ---\n",
    "        F_fit = fit_model.video_params.get('num_frames', 4)\n",
    "        T = video.shape[1]\n",
    "        if T < F_fit:\n",
    "            # 0-패딩으로 프레임 보충\n",
    "            pad = F_fit - T\n",
    "            pad_zeros = torch.zeros(video.size(0), pad, *video.shape[2:], device=video.device, dtype=video.dtype)\n",
    "            video_fit = torch.cat([video, pad_zeros], dim=1)\n",
    "        else:\n",
    "            video_fit = video[:, :F_fit]\n",
    "\n",
    "        video_fit = video_fit.contiguous()\n",
    "\n",
    "        # --- FiT: text & (4프레임) video ---\n",
    "        data_fit = {'text': toks, 'video': video_fit}   # ← 여기!\n",
    "        t_fit, v_fit = fit_model(data_fit)\n",
    "        t_fit = F.normalize(t_fit, dim=-1)\n",
    "        v_fit = F.normalize(v_fit, dim=-1)\n",
    "\n",
    "        # --- V-JEPA2: 32프레임 그대로 사용 ---\n",
    "        v_jepa = vjepa_model.compute_video(video)       # (B, Dj)\n",
    "        v_jepa = F.normalize(v_jepa, dim=-1)\n",
    "\n",
    "        # CPU로 모아서 나중에 cat (메모리 안전)\n",
    "        txt_emb_list.append(t_fit.cpu())\n",
    "        vid_fit_list.append(v_fit.cpu())\n",
    "        vid_jepa_list.append(v_jepa.cpu())\n",
    "\n",
    "txt_emb = torch.cat(txt_emb_list, dim=0)   # [N, D]\n",
    "vid_fit = torch.cat(vid_fit_list, dim=0)   # [N, D]\n",
    "vid_jepa = torch.cat(vid_jepa_list, dim=0) # [N, Dj]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4837fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Block 5: V‑JEPA2 임베딩 표준화 + L2 정규화 =====\n",
    "# 데이터셋 통계로 간단 표준화 (평균/표준편차); 그 다음 L2 normalize\n",
    "vj_mean = vid_jepa.mean(dim=0, keepdim=True)\n",
    "vj_std  = vid_jepa.std(dim=0, keepdim=True) + 1e-6\n",
    "vid_jepa_std = (vid_jepa - vj_mean) / vj_std\n",
    "vid_jepa_std = F.normalize(vid_jepa_std, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58826f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "ALPHA = 0.7  # 가중합 비율 (필요시 조정)\n",
    "\n",
    "# 현재:\n",
    "#   txt_emb: [N, 256]  (샘플별 텍스트 임베딩)\n",
    "#   vid_fit: [N, 256]  (샘플별 FiT 비디오 임베딩)\n",
    "#   vid_jepa_256: [N, 256] (샘플별 V-JEPA를 256으로 사상한 임베딩)\n",
    "#   labels: 길이 N, 각 샘플의 action 라벨 문자열\n",
    "\n",
    "# 1) 라벨 → 인덱스들\n",
    "idxs_per_label = defaultdict(list)\n",
    "for i, lab in enumerate(labels):\n",
    "    idxs_per_label[lab].append(i)\n",
    "\n",
    "# 2) 라벨별 텍스트 임베딩 만들기 (평균 + L2 정규화)\n",
    "unique_labels = sorted(list(idxs_per_label.keys()))\n",
    "T_cls = []\n",
    "for lab in unique_labels:\n",
    "    idxs = idxs_per_label[lab]\n",
    "    t_mean = txt_emb[idxs].mean(dim=0, keepdim=True)      # [1, 256]\n",
    "    t_mean = F.normalize(t_mean, dim=-1)\n",
    "    T_cls.append(t_mean)\n",
    "T_cls = torch.cat(T_cls, dim=0).to(txt_emb.device)        # [L, 256], L=유니크 라벨 수\n",
    "\n",
    "# 3) 비디오 임베딩도 L2 정규화 (안 돼있다면)\n",
    "V_fit = F.normalize(vid_fit, dim=-1).to(T_cls.device)         # [N, 256]\n",
    "V_j256 = F.normalize(vid_jepa_256, dim=-1).to(T_cls.device)   # [N, 256]\n",
    "\n",
    "# 4) 라벨-비디오 유사도 계산\n",
    "#   CLIP 스타일: 라벨 텍스트 T_cls ⟷ FiT 비디오 V_fit\n",
    "S_clip_cls = T_cls @ V_fit.T          # [L, N]\n",
    "#   모션 점수:   라벨 텍스트 T_cls ⟷ V-JEPA(256) V_j256\n",
    "S_motion_cls = T_cls @ V_j256.T       # [L, N]\n",
    "#   가중합\n",
    "S_fused_cls = ALPHA * S_clip_cls + (1 - ALPHA) * S_motion_cls  # [L, N]\n",
    "\n",
    "# 5) numpy로 변환\n",
    "sims_np = S_fused_cls.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79265ff9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "쿼리 수(600)와 유니크 라벨 수(60)가 맞지 않음",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m met \u001b[38;5;129;01min\u001b[39;00m metric_fns:\n\u001b[0;32m      8\u001b[0m     name \u001b[38;5;241m=\u001b[39m met\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m----> 9\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mmet\u001b[49m\u001b[43m(\u001b[49m\u001b[43msims_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# sims: [L, N], labels: 길이 N\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     verbose(epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, metrics\u001b[38;5;241m=\u001b[39mres, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m     11\u001b[0m     nested_metrics[name] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\Documents\\Fine-VLA\\src\\model\\metric.py:390\u001b[0m, in \u001b[0;36mntu_t2v_metrics\u001b[1;34m(sims, action_labels, query_masks)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# 액션 라벨들을 유니크하게 정렬\u001b[39;00m\n\u001b[0;32m    389\u001b[0m unique_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(action_labels)))\n\u001b[1;32m--> 390\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_labels) \u001b[38;5;241m==\u001b[39m num_queries, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m쿼리 수(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_queries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)와 유니크 라벨 수(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)가 맞지 않음\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# 각 라벨별로 해당하는 비디오 인덱스들 구하기\u001b[39;00m\n\u001b[0;32m    393\u001b[0m label_to_video_indices \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mAssertionError\u001b[0m: 쿼리 수(600)와 유니크 라벨 수(60)가 맞지 않음"
     ]
    }
   ],
   "source": [
    "from src.model import metric as module_metric\n",
    "from src.trainer.trainer import verbose\n",
    "\n",
    "metric_fns = [getattr(module_metric, m) for m in [\"ntu_t2v_metrics\", \"ntu_v2t_metrics\"]]\n",
    "\n",
    "nested_metrics = {}\n",
    "for met in metric_fns:\n",
    "    name = met.__name__\n",
    "    res = met(sims_np, labels, query_masks=None)   # sims: [L, N], labels: 길이 N\n",
    "    verbose(epoch=0, metrics=res, name=\"\", mode=name)\n",
    "    nested_metrics[name] = res\n",
    "\n",
    "nested_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e1203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fine-VLA",
   "language": "python",
   "name": "fine-vla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
