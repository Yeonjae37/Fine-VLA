{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b37a8b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121 Windows\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 1: Setup & imports =====\n",
    "import os, sys, platform, pathlib, math, time, copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# repo-root import (프로젝트 루트 경로를 sys.path에 올리는 헬퍼)\n",
    "import autorootcwd\n",
    "\n",
    "# Windows에서 pickled checkpoint 로드시 PosixPath 이슈 회피\n",
    "if platform.system() == 'Windows':\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "# parse_config 모듈을 직접 import\n",
    "import scripts.parse_config as _pc\n",
    "sys.modules['parse_config'] = _pc  # 전역 네임스페이스에 등록\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 프로젝트 모듈\n",
    "from src.data.data_loader import TextVideoDataLoader\n",
    "from src.model.model import FrozenInTime, compute_similarity\n",
    "from src.model import metric as module_metric\n",
    "from src.trainer.trainer import verbose\n",
    "from src.utils.util import state_dict_data_parallel_fix\n",
    "\n",
    "print(torch.__version__, platform.system())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8640e04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 2: Configuration =====\n",
    "\n",
    "# --- Data settings (test.py와 동일 split/test 변환) ---\n",
    "DL_KW = dict(\n",
    "    dataset_name=\"NTU\",\n",
    "    text_params={\"input\": \"text\"},\n",
    "    # ✔ FiT ckpt가 4-frame STFormer라면 dataloader도 4로 맞추는 게 가장 깔끔\n",
    "    video_params={\"extraction_fps\": 25, \"extraction_res\": 320, \"input_res\": 224, \"num_frames\": 4, \"stride\": 1},\n",
    "    data_dir=\"data/nturgbd\",\n",
    "    metadata_dir=\"data/nturgbd\",\n",
    "    split=\"test\",\n",
    "    tsfm_params={\"input_res\": 224, \"center_crop\": 224},\n",
    "    tsfm_split=\"test\",\n",
    "    subsample=1,\n",
    "    sliding_window_stride=-1,\n",
    "    reader=\"decord\",\n",
    "    batch_size=8,\n",
    "    num_workers=2,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "TEXT_MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "# --- Backbone 1: Frozen-in-Time (SpaceTimeTransformer) ---\n",
    "FIT_ARGS = dict(\n",
    "    video_params={\"model\": \"SpaceTimeTransformer\",\n",
    "                  \"arch_config\":\"base_patch16_224\",\n",
    "                  \"num_frames\": 4,                     # ckpt와 정확히 일치\n",
    "                  \"vit_init\":\"imagenet-21k\",\n",
    "                  \"attention_style\":\"frozen-in-time\",\n",
    "                  \"pretrained\": True},\n",
    "    text_params={\"model\": TEXT_MODEL_NAME, \"pretrained\": True},\n",
    "    projection_dim=256,\n",
    ")\n",
    "\n",
    "# --- Backbone 2: V-JEPA2 (모션 민감 표현)\n",
    "# HF 접근이 막힐 수 있으면, 너의 FrozenInTime(VJEPA2) 구현에서 로컬 가중치 경로를 쓰도록 해둔 버전이면 그대로 동작함.\n",
    "USE_VJEPA = True\n",
    "VJEPA_ARGS = dict(\n",
    "    video_params={\"model\": \"VJEPA2\", \"num_frames\": 32, \"pretrained\": True},\n",
    "    text_params={\"model\": TEXT_MODEL_NAME, \"pretrained\": True},\n",
    "    projection_dim=256,\n",
    ")\n",
    "\n",
    "# --- FiT checkpoint (4-frame STFormer) ---\n",
    "FIT_CKPT_PATH = \"src/exps/pretrained/cc-webvid2m-4f_stformer_b_16_224.pth.tar\"  # 필요 시 수정\n",
    "\n",
    "# --- Fusion settings ---\n",
    "FUSION_MODE = \"weighted\"   # \"weighted\" 또는 \"rrf\"\n",
    "ALPHA       = 0.85         # weighted일 때 FiT(텍스트정렬) 비중\n",
    "TAU_CLIP    = 0.07         # FiT 점수 온도\n",
    "TAU_MOT     = 0.07         # JEPA->256 점수 온도\n",
    "RIDGE_LAMBDA = 1e-3        # JEPA->256 사상 릿지 정규화\n",
    "\n",
    "# --- Loop control ---\n",
    "MAX_BATCHES = None         # 빠른 테스트용으로 정수로 제한 가능 (예: 100)\n",
    "\n",
    "print('Config OK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "817010b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 600\n",
      "Tokenizer loaded: distilbert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 3: DataLoader & Tokenizer =====\n",
    "data_loader = TextVideoDataLoader(**DL_KW)\n",
    "print('Dataset length:', len(data_loader.dataset))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "print('Tokenizer loaded:', TEXT_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "540bed9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######USING ATTENTION STYLE:  frozen-in-time\n",
      "[V-JEPA2] model ready\n",
      "Loading FiT checkpoint: src/exps/pretrained/cc-webvid2m-4f_stformer_b_16_224.pth.tar\n",
      "[FiT] checkpoint loaded strictly\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4: Load models =====\n",
    "fit_model = FrozenInTime(**FIT_ARGS).to(device).eval()\n",
    "\n",
    "vjepa_model = None\n",
    "if USE_VJEPA:\n",
    "    try:\n",
    "        vjepa_model = FrozenInTime(**VJEPA_ARGS).to(device).eval()\n",
    "        print('[V-JEPA2] model ready')\n",
    "    except Exception as e:\n",
    "        print('[V-JEPA2] init failed:', e)\n",
    "        vjepa_model = None\n",
    "\n",
    "# FiT checkpoint 로드 (4-frame STFormer과 strict하게 맞춤)\n",
    "print('Loading FiT checkpoint:', FIT_CKPT_PATH)\n",
    "ckpt = torch.load(FIT_CKPT_PATH, map_location='cpu', weights_only=False)\n",
    "\n",
    "sd = state_dict_data_parallel_fix(ckpt['state_dict'], fit_model.state_dict())\n",
    "missing, unexpected = fit_model.load_state_dict(sd, strict=False)\n",
    "if missing or unexpected:\n",
    "    print('[FiT] load_state_dict non-strict:', 'missing', len(missing), 'unexpected', len(unexpected))\n",
    "else:\n",
    "    print('[FiT] checkpoint loaded strictly')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "472e83ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 75/75 [01:20<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> txt: (600, 256) fit_vid: (600, 256) jepa_vid: (600, 256)\n",
      "Samples (labels): 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 5: Extract embeddings (FiT text/video, V-JEPA2 video) =====\n",
    "txt_emb_list, vid_fit_list, vid_jepa_list, labels = [], [], [], []\n",
    "\n",
    "fit_frames = FIT_ARGS['video_params'].get('num_frames', 4)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, batch in enumerate(tqdm(data_loader, desc='Embedding')):\n",
    "        if MAX_BATCHES is not None and bi >= MAX_BATCHES:\n",
    "            break\n",
    "\n",
    "        # 텍스트\n",
    "        texts = batch['text']\n",
    "        toks = tokenizer(texts, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "        # 비디오\n",
    "        video = batch['video'].to(device)                     # (B, T, C, H, W)\n",
    "        video_fit = video[:, :fit_frames].contiguous()        # ✔ FiT가 기대하는 프레임 이하로 보장 + contiguous()\n",
    "\n",
    "        # 라벨\n",
    "        caps = batch['meta']['raw_captions']\n",
    "        if isinstance(caps, list):\n",
    "            labels.extend(caps)\n",
    "        else:\n",
    "            labels.append(caps)\n",
    "\n",
    "        # FiT: text & video\n",
    "        out_t, out_v = fit_model({'text': toks, 'video': video_fit})\n",
    "        out_t = F.normalize(out_t, dim=-1)\n",
    "        out_v = F.normalize(out_v, dim=-1)\n",
    "        txt_emb_list.append(out_t.cpu())\n",
    "        vid_fit_list.append(out_v.cpu())\n",
    "\n",
    "        # V-JEPA2: video only\n",
    "        if vjepa_model is not None:\n",
    "            vj = vjepa_model.compute_video(video)  # 너의 FrozenInTime 구현에서 내부적으로 V-JEPA2 경로 처리\n",
    "            vid_jepa_list.append(vj.cpu())\n",
    "\n",
    "txt_emb  = torch.cat(txt_emb_list, dim=0)    # [N, 256]\n",
    "vid_fit  = torch.cat(vid_fit_list, dim=0)    # [N, 256]\n",
    "vid_jepa = torch.cat(vid_jepa_list, dim=0) if len(vid_jepa_list) > 0 else None\n",
    "\n",
    "print('Shapes -> txt:', tuple(txt_emb.shape), 'fit_vid:', tuple(vid_fit.shape), 'jepa_vid:', None if vid_jepa is None else tuple(vid_jepa.shape))\n",
    "print('Samples (labels):', len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86b44c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JEPA->256: (600, 256)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6: JEPA -> 256 mapping (ridge regression with centering) =====\n",
    "vid_jepa_256 = None\n",
    "W = None\n",
    "\n",
    "if vid_jepa is not None:\n",
    "    X = vid_jepa.to(device)           # [N, Dj]   (JEPA 원공간)\n",
    "    Y = vid_fit.to(device)            # [N, 256]  (FiT 비디오 임베딩)\n",
    "\n",
    "    # 중앙화(centering)\n",
    "    Xm = X.mean(dim=0, keepdim=True)\n",
    "    Ym = Y.mean(dim=0, keepdim=True)\n",
    "    Xc = X - Xm\n",
    "    Yc = Y - Ym\n",
    "\n",
    "    lam = RIDGE_LAMBDA\n",
    "    XT = Xc.t()\n",
    "    XtX = XT @ Xc\n",
    "    reg = lam * torch.eye(XtX.size(0), device=X.device, dtype=X.dtype)\n",
    "    W = torch.linalg.solve(XtX + reg, XT @ Yc)  # [Dj, 256]\n",
    "\n",
    "    vid_jepa_256 = (X - Xm) @ W + Ym\n",
    "    vid_jepa_256 = F.normalize(vid_jepa_256, dim=-1)\n",
    "\n",
    "print('JEPA->256:', None if vid_jepa_256 is None else tuple(vid_jepa_256.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bad232b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score shapes: (600, 600) (600, 600) (600, 600)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 7: Score computation & fusion =====\n",
    "\n",
    "def zscore_rows(M: torch.Tensor) -> torch.Tensor:\n",
    "    mu = M.mean(dim=1, keepdim=True)\n",
    "    sd = M.std(dim=1, keepdim=True).clamp_min(1e-6)\n",
    "    return (M - mu) / sd\n",
    "\n",
    "def rrf(sim: torch.Tensor, k: int = 60) -> torch.Tensor:\n",
    "    # Reciprocal Rank Fusion (스케일 독립적 late fusion)\n",
    "    ranks = sim.argsort(dim=1, descending=True)\n",
    "    rrf_scores = torch.zeros_like(sim)\n",
    "    N = sim.size(1)\n",
    "    inv = 1.0 / (k + torch.arange(1, N+1, device=sim.device, dtype=sim.dtype))\n",
    "    for i in range(sim.size(0)):\n",
    "        rrf_scores[i, ranks[i]] = inv\n",
    "    return rrf_scores\n",
    "\n",
    "txt_n = F.normalize(txt_emb.to(device), dim=-1)\n",
    "fit_n = F.normalize(vid_fit.to(device), dim=-1)\n",
    "\n",
    "# (1) CLIP-like score (FiT text ↔ FiT video)\n",
    "S_clip = (txt_n @ fit_n.t()) / TAU_CLIP\n",
    "\n",
    "# (2) Motion score (FiT text ↔ (JEPA→256) video)\n",
    "if vid_jepa_256 is not None:\n",
    "    S_motion = (txt_n @ vid_jepa_256.t()) / TAU_MOT\n",
    "    if FUSION_MODE == \"weighted\":\n",
    "        S_fused = ALPHA * zscore_rows(S_clip) + (1 - ALPHA) * zscore_rows(S_motion)\n",
    "    elif FUSION_MODE == \"rrf\":\n",
    "        S_fused = rrf(S_clip) + rrf(S_motion)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown FUSION_MODE: {FUSION_MODE}\")\n",
    "else:\n",
    "    print('[WARN] No JEPA embeddings — using FiT-only scores.')\n",
    "    S_motion = None\n",
    "    S_fused = S_clip\n",
    "\n",
    "print('Score shapes:', tuple(S_clip.shape), None if S_motion is None else tuple(S_motion.shape), tuple(S_fused.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8da1877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes(label-level): torch.Size([60, 600]) torch.Size([60, 600])\n",
      "[ntu_t2v_metrics] epoch 0, R@1: 10.0, R@5: 25.0, R@10 36.7, R@50 78.3MedR: 20, MeanR: 32.8\n",
      "[ntu_v2t_metrics] epoch 0, R@1: 5.2, R@5: 15.5, R@10 25.8, R@50 88.2MedR: 23, MeanR: 25.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ntu_t2v_metrics': {'R1': np.float64(10.0),\n",
       "  'R5': np.float64(25.0),\n",
       "  'R10': np.float64(36.666666666666664),\n",
       "  'R50': np.float64(78.33333333333333),\n",
       "  'MedR': np.float64(20.0),\n",
       "  'MeanR': np.float64(32.766666666666666),\n",
       "  'query_ranks': array([  7,  35, 155,  87,  20,   9,   3,  15,  53,  43,  49,  20,  23,\n",
       "           2,  55,   8,  10,   1,  43,  42,  52,  54,  18,  38,  43,   3,\n",
       "          14,  23, 332,  40,  53,  72,   8,   1,   1,  25,  24,  21,   2,\n",
       "           7, 111,  55,  12,   7,  12,  30,  17,   2,   2,   1,  14,  29,\n",
       "          54,   5,  68,   2,   1,  28,   4,   1])},\n",
       " 'ntu_v2t_metrics': {'R1': np.float64(5.166666666666667),\n",
       "  'R5': np.float64(15.5),\n",
       "  'R10': np.float64(25.833333333333336),\n",
       "  'R50': np.float64(88.16666666666667),\n",
       "  'MedR': np.float64(23.0),\n",
       "  'MeanR': np.float64(25.421666666666667),\n",
       "  'query_ranks': array([21, 10, 38, 57, 42, 17,  3, 21, 21, 35, 38, 23, 53, 30, 42,  5,  1,\n",
       "         37, 35,  8, 53, 55, 28, 10, 49, 39, 53, 39, 55, 58, 36, 26, 32, 24,\n",
       "         37, 22, 51, 37, 50, 59, 19, 32, 51, 35, 44, 57, 51, 11, 19, 23,  8,\n",
       "         58, 41,  7, 54, 49, 52, 21,  5, 47, 32, 12, 10, 14, 24,  3, 22, 11,\n",
       "         60, 36, 27,  1, 47, 10, 24,  2,  7, 25, 12, 43, 14, 11, 30, 28, 51,\n",
       "          9, 18,  9, 22,  5, 18, 36,  2, 48,  7,  9, 12, 60, 59, 38, 30,  3,\n",
       "         14,  9,  6, 13,  6, 38,  9,  5,  1,  8, 34,  4, 31, 32, 38,  7, 50,\n",
       "         22, 19, 27, 10,  3, 31, 33, 48, 13, 26, 45, 29, 36,  2, 57, 42, 32,\n",
       "         13, 18, 27,  1, 15, 47,  2, 43, 59, 11,  8,  6, 17, 21, 11, 14, 30,\n",
       "         56, 53, 21, 21, 35, 11, 15,  9, 28, 34, 53, 29, 21, 27, 29,  3, 17,\n",
       "         52, 37,  6, 11, 27, 12,  1, 13, 38, 39, 37, 43,  8,  2, 38, 17,  2,\n",
       "         18, 44, 53, 11,  2,  1, 32, 21,  7, 30, 14,  2,  7,  4, 54,  5, 23,\n",
       "         42, 18, 29,  2,  2,  2, 13, 22, 20, 50,  1, 24, 21, 28,  1,  7, 41,\n",
       "         44, 11, 17,  8,  6, 12, 57, 36, 35,  5, 22, 16, 48,  5, 34, 34, 53,\n",
       "         32, 26,  1, 52, 13, 51, 51, 53, 45, 14, 50, 17,  7, 32, 13, 60, 23,\n",
       "         59,  1, 32, 20,  4,  5, 37, 52, 33, 14, 46, 54, 27, 16, 22, 52,  2,\n",
       "         13, 18, 17, 20,  7, 17,  4, 40, 40, 18, 16, 34,  5, 22,  1, 41, 42,\n",
       "         58, 53,  1, 57,  1, 23, 12,  7, 26, 12, 23, 20, 53,  1, 46,  1, 30,\n",
       "         48, 32, 45, 10, 25, 28,  7, 55,  2, 37,  3, 34, 12, 55, 31, 39, 50,\n",
       "         17, 52, 57, 44,  2,  5, 58, 40, 14, 43,  7, 37,  3, 24, 58, 29, 26,\n",
       "         50, 40, 31, 12, 15, 15, 59, 11, 26,  1, 33, 39,  6, 10, 18, 14, 37,\n",
       "         20, 37, 40, 27,  4, 16, 28,  6,  9, 15, 52, 42, 30, 33, 42, 28, 57,\n",
       "         18, 39, 58,  6, 29, 12, 60, 15, 40, 17, 46, 25, 21, 49, 22,  7,  8,\n",
       "         17,  5, 41, 23, 49, 48, 42, 41, 36, 48,  3, 33, 31,  2, 34, 16, 58,\n",
       "         15, 44, 17, 27, 44, 15, 44, 28, 49, 16,  3,  1, 18, 17, 45,  1, 22,\n",
       "         20, 46, 20,  9,  6, 60,  9, 35,  3, 36, 32, 16, 33, 12, 50, 21,  8,\n",
       "          3, 52, 49, 44, 37, 23, 42, 53, 40,  1, 12, 44, 45, 20, 42, 35, 41,\n",
       "         55, 52,  2,  4, 25, 46, 16, 30, 43, 37, 59, 57, 21, 20, 23, 31, 10,\n",
       "          7, 54, 33, 46, 19,  3, 25, 30, 22, 22, 24, 27, 27, 59,  7, 23, 20,\n",
       "         53, 32, 51, 16, 20, 25, 11,  9, 22, 24, 39, 39, 52, 27, 26, 32, 27,\n",
       "         13, 17, 26, 43, 46, 42, 44, 37, 30, 17, 14,  5, 13, 13, 11,  7, 11,\n",
       "         37, 15, 13, 35, 52, 44, 36, 43, 51, 55, 49, 59, 56,  5,  1, 12,  3,\n",
       "          1,  1,  2, 45,  3,  2,  6,  4,  3,  1,  4,  2,  1, 32, 18,  7, 19,\n",
       "         31,  6, 45, 31, 26, 37,  9, 30, 40,  3, 15,  2,  5, 11,  2,  6,  9,\n",
       "         37, 21, 20, 14,  6, 14, 18, 16,  7,  6,  1,  1,  1,  4,  1,  2,  1,\n",
       "          2,  1,  7,  6,  1])}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === (Fix) NTU: 텍스트를 라벨별 평균으로 60개 쿼리로 축약 ===\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1) 라벨 인덱스 모음\n",
    "unique_labels = sorted(set(labels))\n",
    "lab2idx = {lab: [] for lab in unique_labels}\n",
    "for i, lab in enumerate(labels):\n",
    "    lab2idx[lab].append(i)\n",
    "\n",
    "# 2) 라벨별 텍스트 임베딩 평균 (기존 txt_emb는 인스턴스 단위)\n",
    "txt_lab = []\n",
    "for lab in unique_labels:\n",
    "    idxs = torch.tensor(lab2idx[lab], dtype=torch.long)\n",
    "    txt_lab.append(txt_emb[idxs].mean(dim=0))\n",
    "txt_lab = torch.stack(txt_lab, dim=0)   # [L=60, 256]\n",
    "txt_lab = F.normalize(txt_lab, dim=-1).to(device)\n",
    "\n",
    "# 3) 비디오 임베딩 정규화(이미 되어 있으면 생략 가능)\n",
    "fit_n = F.normalize(vid_fit.to(device), dim=-1)\n",
    "# vjepa_256이 없는 경우도 처리\n",
    "vjepa_n = F.normalize(vid_jepa_256.to(device), dim=-1) if 'vid_jepa_256' in globals() and vid_jepa_256 is not None else None\n",
    "\n",
    "# 4) 라벨-수준 점수 재계산\n",
    "S_clip_lab = (txt_lab @ fit_n.t()) / TAU_CLIP                # [60, N]\n",
    "if vjepa_n is not None:\n",
    "    S_motion_lab = (txt_lab @ vjepa_n.t()) / TAU_MOT         # [60, N]\n",
    "    if FUSION_MODE == \"weighted\":\n",
    "        # z-score는 행(각 쿼리) 단위로\n",
    "        mu1, sd1 = S_clip_lab.mean(dim=1, keepdim=True), S_clip_lab.std(dim=1, keepdim=True).clamp_min(1e-6)\n",
    "        mu2, sd2 = S_motion_lab.mean(dim=1, keepdim=True), S_motion_lab.std(dim=1, keepdim=True).clamp_min(1e-6)\n",
    "        S_fused_lab = ALPHA * (S_clip_lab - mu1) / sd1 + (1 - ALPHA) * (S_motion_lab - mu2) / sd2\n",
    "    elif FUSION_MODE == \"rrf\":\n",
    "        def rrf(sim, k=60):\n",
    "            ranks = sim.argsort(dim=1, descending=True)\n",
    "            out = torch.zeros_like(sim)\n",
    "            N = sim.size(1)\n",
    "            inv = 1.0 / (k + torch.arange(1, N+1, device=sim.device, dtype=sim.dtype))\n",
    "            for i in range(sim.size(0)):\n",
    "                out[i, ranks[i]] = inv\n",
    "            return out\n",
    "        S_fused_lab = rrf(S_clip_lab) + rrf(S_motion_lab)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown FUSION_MODE: {FUSION_MODE}\")\n",
    "else:\n",
    "    S_fused_lab = S_clip_lab\n",
    "\n",
    "print('Shapes(label-level):', S_clip_lab.shape, S_fused_lab.shape)  # [60, N]\n",
    "\n",
    "# 5) 메트릭 입력은 라벨-수준 유사도와, 비디오별 라벨 목록\n",
    "sims_np = S_fused_lab.detach().cpu().numpy()\n",
    "action_labels = labels  # 길이 N (비디오 열에 해당)\n",
    "\n",
    "# 그대로 metric 계산 실행\n",
    "from src.model import metric as module_metric\n",
    "from src.trainer.trainer import verbose\n",
    "\n",
    "metric_fns = [getattr(module_metric, m) for m in [\"ntu_t2v_metrics\", \"ntu_v2t_metrics\"]]\n",
    "nested_metrics = {}\n",
    "for met in metric_fns:\n",
    "    name = met.__name__\n",
    "    res = met(sims_np, action_labels, query_masks=None)\n",
    "    verbose(epoch=0, metrics=res, name=\"\", mode=name)\n",
    "    nested_metrics[name] = res\n",
    "\n",
    "nested_metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fine-VLA",
   "language": "python",
   "name": "fine-vla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
