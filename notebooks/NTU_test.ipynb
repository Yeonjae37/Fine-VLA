{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447759b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "root = r'C:\\Users\\user\\Documents\\Fine-VLA'\n",
    "\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)\n",
    "\n",
    "os.chdir(root)\n",
    "\n",
    "import data.data_loader as module_data\n",
    "import model.metric as module_metric\n",
    "import model.model as module_arch\n",
    "from model.model import compute_similarity\n",
    "from scripts.parse_config import ConfigParser\n",
    "from trainer.trainer import verbose\n",
    "from utils.util import state_dict_data_parallel_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "38fa0f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from sacred import Experiment\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6c18d648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--batch_size'], dest='batch_size', nargs=None, const=None, default=16, type=<class 'int'>, choices=None, required=False, help='size of batch', metavar=None)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "original_argv = sys.argv.copy()\n",
    "sys.argv = ['ipython']\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Template')\n",
    "\n",
    "parser.add_argument('-r', '--resume', default=None, type=str,\n",
    "                    help='path to latest checkpoint (default: None)')\n",
    "parser.add_argument('-d', '--device', default=None, type=str,\n",
    "                    help='indices of GPUs to enable (default: all)')\n",
    "parser.add_argument('-c', '--config', default=None, type=str,\n",
    "                    help='config file path (default: None)')\n",
    "parser.add_argument('-s', '--sliding_window_stride', default=-1, type=int,\n",
    "                    help='test time temporal augmentation, repeat samples with different start times.')\n",
    "parser.add_argument('--save_feats', default=None,\n",
    "                    help='path to store text & video feats, this is for saving embeddings if you want to do offline retrieval.')\n",
    "parser.add_argument('--save_type', default='both', choices=['both', 'text', 'video'],\n",
    "                    help='Whether to save video, text or both feats. If running on inference videos, text is just a placeholder')\n",
    "parser.add_argument('--vis_token_similarity', action='store_true')\n",
    "parser.add_argument('--split', default='test', choices=['train', 'val', 'test'],\n",
    "                    help='split to evaluate on.')\n",
    "parser.add_argument('--batch_size', default=16, type=int,\n",
    "                    help='size of batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0bf3c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv.extend([\n",
    "    '-c', 'configs/ntu.json',\n",
    "    '-r', 'exps/pretrained/cc-webvid2m-4f_stformer_b_16_224.pth.tar',\n",
    "    '--split', 'test',\n",
    "    '--batch_size', '16'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "af7ec7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: NTU\n",
      "Data dir: C:/Users/user/Documents/data/nturgbd_rgb\n",
      "Cut: standard\n"
     ]
    }
   ],
   "source": [
    "config = ConfigParser(parser, test=True) # parse config\n",
    "\n",
    "print(f\"Dataset name: {config['data_loader']['args']['dataset_name']}\")\n",
    "print(f\"Data dir: {config['data_loader']['args']['data_dir']}\")\n",
    "print(f\"Cut: {config['data_loader']['args']['cut']}\")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "44967807",
   "metadata": {},
   "outputs": [],
   "source": [
    "config._config['data_loader']['args']['split'] = args.split\n",
    "config._config['data_loader']['args']['tsfm_split'] = 'test'\n",
    "config._config['data_loader']['args']['shuffle'] = False\n",
    "config._config['data_loader']['args']['batch_size'] = args.batch_size\n",
    "config._config['data_loader']['args']['sliding_window_stride'] = args.sliding_window_stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7141100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextVideoDataLoader\n"
     ]
    }
   ],
   "source": [
    "data_loader = config.initialize('data_loader', module_data)\n",
    "# config['data_loader']['type'] -> \"TextVideoDataLoader\"\n",
    "# getattr(module_data, config['data_loader']['type'])\n",
    "# TextVideoDataLoader(**module_args) -> data_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5c9bea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextVideoDataLoader\n",
      "  dataset_name: NTU\n",
      "  data_dir: C:/Users/user/Documents/data/nturgbd_rgb\n",
      "  shuffle: False\n",
      "  num_workers: 16\n",
      "  batch_size: 16\n",
      "  split: test\n",
      "  cut: standard\n",
      "  subsample: 1\n",
      "  text_params: ['input']\n",
      "  video_params: ['extraction_fps', 'extraction_res', 'input_res', 'num_frames', 'stride']\n",
      "  tsfm_split: test\n",
      "  sliding_window_stride: -1\n"
     ]
    }
   ],
   "source": [
    "module_name = config['data_loader']['type']\n",
    "print(module_name)\n",
    "\n",
    "module_args = dict(config['data_loader']['args'])\n",
    "for key, value in module_args.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  {key}: {list(value.keys())}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "809732fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader.NTU_dataset import NTU\n",
    "from data_loader.transforms import init_transform_dict\n",
    "\n",
    "dataset_name = 'NTU'\n",
    "text_params = config['data_loader']['args']['text_params']\n",
    "video_params = config['data_loader']['args']['video_params']\n",
    "data_dir = config['data_loader']['args']['data_dir']\n",
    "metadata_dir = config['data_loader']['args'].get('metadata_dir')\n",
    "split = config['data_loader']['args']['split']\n",
    "cut = config['data_loader']['args'].get('cut')\n",
    "subsample = config['data_loader']['args'].get('subsample', 1)\n",
    "sliding_window_stride = config['data_loader']['args'].get('sliding_window_stride', -1)\n",
    "reader = config['data_loader']['args'].get('reader', 'decord')\n",
    "\n",
    "tsfm_params = config['data_loader']['args'].get('tsfm_params', {})\n",
    "tsfm_dict = init_transform_dict(**tsfm_params)\n",
    "tsfm_split = config['data_loader']['args'].get('tsfm_split', split)\n",
    "tsfm = tsfm_dict[tsfm_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "209c6278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<data_loader.NTU_dataset.NTU object at 0x000001D88613D120>\n",
      "dataset size: 600\n",
      "metadata shape: (600,)\n",
      "first video id: S017C001P008R001A001_rgb\n",
      "dataset_name: NTU\n",
      "split: test\n",
      "data directory: C:/Users/user/Documents/data/nturgbd_rgb\n",
      "metadata directory: C:/Users/user/Documents/data/nturgbd_rgb\n"
     ]
    }
   ],
   "source": [
    "ntu_dataset = NTU(\n",
    "    dataset_name=dataset_name,\n",
    "    text_params=text_params, \n",
    "    video_params=video_params,\n",
    "    data_dir=data_dir,\n",
    "    metadata_dir=metadata_dir,\n",
    "    split=split,\n",
    "    tsfms=tsfm,\n",
    "    cut=cut,\n",
    "    subsample=subsample,\n",
    "    sliding_window_stride=sliding_window_stride,\n",
    "    reader=reader\n",
    ")\n",
    "\n",
    "print(ntu_dataset)\n",
    "print(f\"dataset size: {len(ntu_dataset)}\")\n",
    "print(f\"metadata shape: {ntu_dataset.metadata.shape}\")\n",
    "print(f\"first video id: {ntu_dataset.metadata.index[0]}\")\n",
    "print(f\"dataset_name: {ntu_dataset.dataset_name}\")\n",
    "print(f\"split: {ntu_dataset.split}\")\n",
    "print(f\"data directory: {ntu_dataset.data_dir}\")\n",
    "print(f\"metadata directory: {ntu_dataset.metadata_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254dec8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    videoid\n",
      "0  S017C001P003R001A001_rgb\n",
      "1  S017C001P003R001A002_rgb\n",
      "2  S017C001P003R001A003_rgb\n",
      "3  S017C001P003R001A004_rgb\n",
      "4  S017C001P003R001A005_rgb\n",
      "                    videoid\n",
      "0  S017C001P008R001A001_rgb\n",
      "1  S017C001P008R001A002_rgb\n",
      "2  S017C001P008R001A003_rgb\n",
      "3  S017C001P008R001A004_rgb\n",
      "4  S017C001P008R001A005_rgb\n"
     ]
    }
   ],
   "source": [
    "ntu_metadata_dir = \"data/nturgbd\"\n",
    "csv_fp = os.path.join(ntu_metadata_dir, 'annotations.csv')\n",
    "ntu_df = pd.read_csv(csv_fp)\n",
    "\n",
    "ntu_split_dir = os.path.join(ntu_metadata_dir, 'splits')\n",
    "ntu_train_list_path = \"train_list.txt\"\n",
    "ntu_test_list_path = \"test_list.txt\"\n",
    "\n",
    "        \n",
    "ntu_train_df = pd.read_csv(os.path.join(ntu_split_dir, ntu_train_list_path), names=['videoid'])\n",
    "ntu_test_df = pd.read_csv(os.path.join(ntu_split_dir, ntu_test_list_path), names=['videoid'])\n",
    "\n",
    "print(ntu_train_df.head())\n",
    "print(ntu_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6a90e194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_id\n",
      "S017C001P003R001A001_rgb    [drink_water]\n",
      "S017C001P003R001A002_rgb       [eat_meal]\n",
      "S017C001P003R001A003_rgb    [brush_teeth]\n",
      "S017C001P003R001A004_rgb     [brush_hair]\n",
      "S017C001P003R001A005_rgb           [drop]\n",
      "Name: caption, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ntu_metadata = ntu_df.groupby(['video_id'])['caption'].apply(list)\n",
    "print(ntu_metadata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3be37fc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'TextVideoDataLoader' has no attribute 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[195], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTextVideoDataLoader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetadata shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetadata 첫 3행:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'TextVideoDataLoader' has no attribute 'dataset'"
     ]
    }
   ],
   "source": [
    "dataset = TextVideoDataLoader.dataset\n",
    "print(f\"Metadata shape: {dataset.metadata.shape}\")\n",
    "print(f\"Metadata 첫 3행:\\n{dataset.metadata.head(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9ce2a7ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\user\\Documents\\Fine-VLA\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"c:\\Users\\user\\Documents\\Fine-VLA\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\user\\Documents\\Fine-VLA\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\user\\Documents\\Fine-VLA\\base\\base_dataset.py\", line 85, in __getitem__\n    video_fp, rel_fp = self._get_video_path(sample) # 비디오 경로 가져오기\n  File \"C:\\Users\\user\\Documents\\Fine-VLA\\data_loader\\NTU_dataset.py\", line 37, in _get_video_path\n    rel_path = sample['video_path']\nTypeError: list indices must be integers or slices, not str\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[194], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m first_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\Documents\\Fine-VLA\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\user\\Documents\\Fine-VLA\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1480\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\Documents\\Fine-VLA\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1505\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1505\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\user\\Documents\\Fine-VLA\\.venv\\lib\\site-packages\\torch\\_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\user\\Documents\\Fine-VLA\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"c:\\Users\\user\\Documents\\Fine-VLA\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\user\\Documents\\Fine-VLA\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\user\\Documents\\Fine-VLA\\base\\base_dataset.py\", line 85, in __getitem__\n    video_fp, rel_fp = self._get_video_path(sample) # 비디오 경로 가져오기\n  File \"C:\\Users\\user\\Documents\\Fine-VLA\\data_loader\\NTU_dataset.py\", line 37, in _get_video_path\n    rel_path = sample['video_path']\nTypeError: list indices must be integers or slices, not str\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model_name = config['arch']['args']['text_params']['model']\n",
    "print(\"Text model: {text_model_name}\")\n",
    "\n",
    "if \"openai/clip\" in text_model_name:\n",
    "    tokenizer_builder = transformers.CLIPTokenizer\n",
    "else:\n",
    "    tokenizer_builder = transformers.AutoTokenizer\n",
    "tokenizer = tokenizer_builder.from_pretrained(\n",
    "    text_model_name,\n",
    "    model_max_length=int(config['arch']['args']['text_params'].get('max_length', 1e6)),\n",
    "    TOKENIZERS_PARALLELISM=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "08b8028b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'first_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[196], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[43mfirst_batch\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# text_embeds\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'first_batch' is not defined"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "data = copy.deepcopy(first_batch)\n",
    "\n",
    "# text_embeds\n",
    "\n",
    "if tokenizer is not None:\n",
    "    print(f\"원본 텍스트: {data['text'][:2]}\")\n",
    "    data['text'] = tokenizer(data['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "    print(f\"토큰화 후 키들:{data['text'].keys()}\")\n",
    "    print(f\"Input IDs shape: {data['text']['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 배치 데이터 복사\n",
    "data = copy.deepcopy(first_batch)\n",
    "\n",
    "# 1. 텍스트 토큰화 시각화\n",
    "print(\"\\n===== 텍스트 토큰화 =====\")\n",
    "print(f\"원본 텍스트: {data['text'][:3]}\")\n",
    "\n",
    "if tokenizer is not None:\n",
    "    # 토큰화\n",
    "    data['text'] = tokenizer(data['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "    print(f\"토큰화 후 키들: {data['text'].keys()}\")\n",
    "    print(f\"Input IDs shape: {data['text']['input_ids'].shape}\")\n",
    "    print(f\"Attention mask shape: {data['text']['attention_mask'].shape}\")\n",
    "    \n",
    "    # 토큰 ID 시각화\n",
    "    token_ids = data['text']['input_ids'][:3].numpy()  # 처음 3개 샘플만\n",
    "    \n",
    "    print(\"\\n토큰 ID 매트릭스 (처음 3개 샘플):\")\n",
    "    for i, ids in enumerate(token_ids):\n",
    "        print(f\"샘플 {i+1}: {ids}\")\n",
    "    \n",
    "    # 토큰 디코딩\n",
    "    print(\"\\n토큰 디코딩 결과:\")\n",
    "    for i, ids in enumerate(token_ids):\n",
    "        tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "        print(f\"샘플 {i+1} 토큰: {tokens}\")\n",
    "\n",
    "# 2. 비디오 토큰화 시각화\n",
    "print(\"\\n===== 비디오 프레임 및 패치 시각화 =====\")\n",
    "\n",
    "# 원본 비디오 프레임 시각화\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# 상단 행: 원본 프레임\n",
    "for i in range(min(4, data['video'].shape[1])):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    frame = data['video'][0, i].permute(1, 2, 0).numpy()\n",
    "    if frame.min() < 0 or frame.max() > 1:\n",
    "        frame = (frame - frame.min()) / (frame.max() - frame.min())\n",
    "    plt.imshow(frame)\n",
    "    plt.title(f\"원본 프레임 {i+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "# 하단 행: 패치 그리드 시각화\n",
    "patch_size = 16  # 일반적인 ViT 패치 크기\n",
    "for i in range(min(4, data['video'].shape[1])):\n",
    "    plt.subplot(2, 4, i+5)\n",
    "    frame = data['video'][0, i].permute(1, 2, 0).numpy()\n",
    "    if frame.min() < 0 or frame.max() > 1:\n",
    "        frame = (frame - frame.min()) / (frame.max() - frame.min())\n",
    "    \n",
    "    plt.imshow(frame)\n",
    "    \n",
    "    # 패치 그리드 오버레이\n",
    "    h, w = frame.shape[0], frame.shape[1]\n",
    "    for y in range(0, h, patch_size):\n",
    "        plt.axhline(y=y, color='r', linestyle='-', alpha=0.3)\n",
    "    for x in range(0, w, patch_size):\n",
    "        plt.axvline(x=x, color='r', linestyle='-', alpha=0.3)\n",
    "        \n",
    "    plt.title(f\"프레임 {i+1} 패치 그리드\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. 패치 토큰 샘플 시각화\n",
    "frame_idx = 0\n",
    "frame = data['video'][0, frame_idx].permute(1, 2, 0).numpy()\n",
    "if frame.min() < 0 or frame.max() > 1:\n",
    "    frame = (frame - frame.min()) / (frame.max() - frame.min())\n",
    "\n",
    "h, w = frame.shape[0], frame.shape[1]\n",
    "num_patches_h = h // patch_size\n",
    "num_patches_w = w // patch_size\n",
    "total_patches = num_patches_h * num_patches_w\n",
    "\n",
    "print(f\"\\n프레임당 패치 수: {total_patches} ({num_patches_h}x{num_patches_w})\")\n",
    "print(f\"비디오 패치 토큰 수: {total_patches * data['video'].shape[1]} (프레임당 {total_patches} x {data['video'].shape[1]}프레임)\")\n",
    "\n",
    "# 패치 샘플 추출 및 시각화\n",
    "plt.figure(figsize=(15, 3))\n",
    "patch_indices = [0, total_patches//4, total_patches//2, 3*total_patches//4]\n",
    "\n",
    "for idx, i in enumerate(patch_indices):\n",
    "    row = (i // num_patches_w) * patch_size\n",
    "    col = (i % num_patches_w) * patch_size\n",
    "    \n",
    "    patch = frame[row:row+patch_size, col:col+patch_size, :]\n",
    "    \n",
    "    plt.subplot(1, 4, idx+1)\n",
    "    plt.imshow(patch)\n",
    "    plt.title(f\"패치 토큰 #{i+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle(f\"비디오 패치 토큰 샘플 (프레임 {frame_idx+1})\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fine-VLA",
   "language": "python",
   "name": "fine-vla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
